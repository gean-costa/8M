{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from bson.json_util import dumps\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "\n",
    "import unicodedata\n",
    "import csv\n",
    "\n",
    "from polyglot.downloader import downloader\n",
    "from polyglot.text import Text\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedimentos para baixar as stopwords. Caso já tenha sido feito, ignorar essa etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para a análise de sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader.download(\"embeddings2.pt\")\n",
    "downloader.download(\"sentiment2.pt\")\n",
    "downloader.download(\"morph2.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciando a conexão com o MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.tweets8march"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db.tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1: tweets que não são retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = [\n",
    "    {\n",
    "        '$match': {\n",
    "            'lang': 'pt', \n",
    "            'retweeted_status': {\n",
    "                '$exists': False\n",
    "            }, \n",
    "            'is_quote_status': False\n",
    "        }\n",
    "    }, {\n",
    "        '$project': {\n",
    "            '_id': 1,\n",
    "            'id_tweet': '$id',\n",
    "            'extended_tweet': {\n",
    "                '$ifNull': [\n",
    "                    '$extended_tweet.full_text', '$text'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$project': {\n",
    "            '_id': 1, \n",
    "            'id_tweet': 1,\n",
    "            'text': '$extended_tweet'\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2: tweets que são retweets, porém, apenas os com quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = [\n",
    "    {\n",
    "        '$match': {\n",
    "            'lang': 'pt', \n",
    "            'retweeted_status': {\n",
    "                '$exists': False\n",
    "            }, \n",
    "            'is_quote_status': True\n",
    "        }\n",
    "    }, {\n",
    "        '$project': {\n",
    "            '_id': 1, \n",
    "            'id_tweet': '$id', \n",
    "            'extended_tweet': {\n",
    "                '$ifNull': [\n",
    "                    '$extended_tweet.full_text', '$text'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$project': {\n",
    "            '_id': 1, \n",
    "            'id_tweet': 1, \n",
    "            'text': '$extended_tweet'\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executando a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = list(collection.aggregate(pipeline = pipeline1))\n",
    "document2 = list(collection.aggregate(pipeline = pipeline2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dumps(document1[0:5], indent = 4, sort_keys = True, ensure_ascii = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dumps(document2[0:5], indent = 4, sort_keys = True, ensure_ascii = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = document1 + document2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(['id_tweet'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'].str.replace(r\"http\\S+\",\"\").str.replace(r\"@\\S+\",\"\").str.replace(r\"\\n\",\" \").str.replace(r\"#\\S+\",\" \").str.strip().str.replace(r\"\\s+\",\" \").str.replace(r\"[^\\w\\s]\",\" \")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover links\n",
    "df['text'] = df['text'].str.replace(r\"http\\S+\",\"\") \n",
    "\n",
    "# remover mentions\n",
    "df['text'] = df['text'].str.replace(r\"@\\S+\",\"\")\n",
    "\n",
    "# remover quebra de linhas\n",
    "df['text'] = df['text'].str.replace(r\"\\n\",\" \")\n",
    "\n",
    "# remover hashtags\n",
    "df['text'] = df['text'].str.replace(r\"#\\S+\",\" \")\n",
    "\n",
    "# remover pontuações\n",
    "df['text'] = df['text'].str.replace(r\"[^\\w\\s]\",\" \")\n",
    "\n",
    "# remover espaços duplos\n",
    "df['text'] = df['text'].str.strip().str.replace(r\"\\s{2,}\",\" \")\n",
    "\n",
    "# converter todas as letras para minusculas\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover stopwords da nossa base:\n",
    "def remover_stopwords(texto):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    palavras = [i for i in texto.split() if not i in stopwords]\n",
    "    return (\" \".join(palavras))\n",
    "\n",
    "df['text'] = df['text'].apply(remover_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['text'][0:10].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras = ','.join(list(df['text'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud().generate(palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size = 40).generate(palavras)\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 10000\n",
    "n_components = 10\n",
    "n_top_word = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = df['text']\n",
    "n_samples = len(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df = 0.95, \n",
    "                                min_df =2 ,\n",
    "                                max_features = n_features,\n",
    "                                stop_words = nltk.corpus.stopwords.words('portuguese'))\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "print(\"tf features for LDA extraction is completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting LDA models with tf features\\n\" \n",
    "      \"n_samples = %d and n_features = %d...\" % (n_samples, n_features))\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = n_components, \n",
    "                                max_iter = 5,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 0)\n",
    "\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fiting LSA model\")\n",
    "lsa = TruncatedSVD(n_components=n_components, n_iter=40, tol=0.01)\n",
    "lsa.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTopics in LSA model:\")\n",
    "print_top_words(lsa, tf_feature_names, n_top_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.text.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrair polaridades dos comentários "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity = []\n",
    "\n",
    "for dat in data:\n",
    "    text = Text(dat)\n",
    "    text.language = \"pt\"\n",
    "    try:\n",
    "        polarity.append(text.polarity)\n",
    "    except:\n",
    "        polarity.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando polaridade de todos os tweets em um novo Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = {'text': data, 'polarity': polarity}\n",
    "sentiments_df = pd.DataFrame(data = new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorização dos sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLabel = sentiments_df.polarity.tolist()\n",
    "\n",
    "pos = dataLabel.count(1)\n",
    "neg = dataLabel.count(-1)\n",
    "negMedio = 0\n",
    "posMedio = 0\n",
    "\n",
    "for dat in dataLabel:\n",
    "    if (dat<0 and dat > -1 ):\n",
    "        negMedio += 1\n",
    "    elif(dat > 0 and dat < 1):\n",
    "        posMedio += 1\n",
    "\n",
    "neutral = dataLabel.count(0)\n",
    "label = [\"Positiva\", \"Positiva Média\", \"Negativa\",\"Negativa Média\", \"Neutra\"]\n",
    "\n",
    "print (\"Total Positiva: \", pos)\n",
    "print (\"Total Positiva Média: \", posMedio)\n",
    "print (\"Total Negativa: \", neg)\n",
    "print (\"Total Negativa Média: \", negMedio)\n",
    "print (\"Total Neutra: \", neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "plt.bar(range(len(label)), [pos,posMedio,neg,negMedio,neutral], align ='center')\n",
    "plt.xticks(range(len(label)), label)\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o stemming em nossa base:\n",
    "def stemming(texto):\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    palavras = []\n",
    "    for palavra in texto.split():\n",
    "        palavras.append(stemmer.stem(palavra))\n",
    "    return (\" \".join(palavras))\n",
    "\n",
    "df['text'] = df['text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(instancia):\n",
    "    instancia = re.sub(r\"http\\S+\", \"\", instancia).lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','').replace('\"','')\n",
    "    stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    palavras = [i for i in instancia.split() if not i in stopwords]\n",
    "    return (\" \".join(palavras))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
