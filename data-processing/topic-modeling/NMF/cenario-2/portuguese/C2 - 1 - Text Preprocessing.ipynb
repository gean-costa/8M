{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will look at the steps involved in preprocessing a corpus of unstructed text documents using *scikit-learn*, which we will use later for topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our sample corpus of text, we will use a corpus of news articles collected in 2016. These articles have been stored in a single file and formatted so that one article appears on each line. We will load these articles into a list, and also create a short snippet of text for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T02:53:38.688103Z",
     "start_time": "2021-01-20T02:53:38.595715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 40324 raw text documents\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "raw_documents = []\n",
    "snippets = []\n",
    "with open(os.path.join(\"cenario2_8M2020_tweets_pt.txt\"), \"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        text = line.strip()\n",
    "        raw_documents.append(text)\n",
    "        # keep a short snippet of up to 100 characters as a title for each article\n",
    "        snippets.append(text[0 : min(len(text), 100)])\n",
    "print(\"Read %d raw text documents\" % len(raw_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When preprocessing text, a common approach is to remove non-informative stopwords. The choice of stopwords can have a considerable impact later on. We will use a custom stopword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T02:53:38.704836Z",
     "start_time": "2021-01-20T02:53:38.690025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword list has 234 entries\n"
     ]
    }
   ],
   "source": [
    "custom_stop_words = []\n",
    "\n",
    "with open(\"portuguese.txt\", \"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        custom_stop_words.append(line.strip())\n",
    "        \n",
    "# note that we need to make it hashable\n",
    "print(\"Stopword list has %d entries\" % len(custom_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *bag-of-words model*, each document is represented by a vector in a *m*-dimensional coordinate space, where *m* is number of unique terms across all documents. This set of terms is called the corpus *vocabulary*. \n",
    "\n",
    "Since each document can be represented as a term vector, we can stack these vectors to create a full *document-term matrix*. We can easily create this matrix from a list of document strings using *CountVectorizer* from Scikit-learn. The parameters passed to *CountVectorizer* control the pre-processing steps that it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T02:53:40.093392Z",
     "start_time": "2021-01-20T02:53:38.706738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 40324 X 2580 document-term matrix\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# use a custom stopwords list, set the minimum term-document frequency to 20\n",
    "vectorizer = CountVectorizer(stop_words=custom_stop_words, min_df=20)\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "\n",
    "print(\"Created %d X %d document-term matrix\" % (A.shape[0], A.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process also builds a vocabulary for the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T02:53:40.101223Z",
     "start_time": "2021-01-20T02:53:40.095143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 2580 distinct terms\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save this document-term matrix, terms, and snippets for later use using *Joblib* to persist the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T02:53:40.427908Z",
     "start_time": "2021-01-20T02:53:40.102793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cenario2_tweets-raw.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump((A, terms, snippets), \"cenario2_tweets-raw.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Term Weighting with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the usefulness of the document-term matrix by giving more weight to the more \"important\" terms. The most common normalisation is *term frequencyâ€“inverse document frequency* (TF-IDF). In Scikit-learn, we can generate at TF-IDF weighted document-term matrix by using *TfidfVectorizer* in place of *CountVectorizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T02:53:41.368864Z",
     "start_time": "2021-01-20T02:53:40.429806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 40324 X 2580 TF-IDF-normalized document-term matrix\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# we can pass in the same preprocessing parameters\n",
    "vectorizer = TfidfVectorizer(stop_words=custom_stop_words, min_df=20, max_df=0.9)\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "\n",
    "print(\n",
    "    \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T02:53:41.378312Z",
     "start_time": "2021-01-20T02:53:41.371572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 2580 distinct terms\n"
     ]
    }
   ],
   "source": [
    "# extract the resulting vocabulary\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple characterisation that we might do would be to look at the terms with the highest TF-IDF scores across all documents in the document-term matrix. We can define such a function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T02:53:41.389339Z",
     "start_time": "2021-01-20T02:53:41.380732Z"
    }
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "\n",
    "def rank_terms(A, terms):\n",
    "    # get the sums over each column\n",
    "    sums = A.sum(axis=0)\n",
    "    \n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    \n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0, col]\n",
    "        \n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now display a ranking of the top 20 terms, which gives us a very rough sense of the content of the document collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T02:53:41.409161Z",
     "start_time": "2021-01-20T02:53:41.391484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. mulher (2422.57)\n",
      "02. mulheres (1863.22)\n",
      "03. internacional (1856.94)\n",
      "04. feliz (1526.49)\n",
      "05. parabens (811.17)\n",
      "06. hoje (755.20)\n",
      "07. ser (641.34)\n",
      "08. sororidade (562.44)\n",
      "09. guerreiras (548.05)\n",
      "10. mundo (546.84)\n",
      "11. marco (531.95)\n",
      "12. luta (516.62)\n",
      "13. dias (463.10)\n",
      "14. amo (426.22)\n",
      "15. sempre (407.32)\n",
      "16. tudo (407.24)\n",
      "17. video (391.80)\n",
      "18. vida (383.71)\n",
      "19. respeito (383.23)\n",
      "20. homenagem (360.98)\n"
     ]
    }
   ],
   "source": [
    "ranking = rank_terms(A, terms)\n",
    "\n",
    "for i, pair in enumerate(ranking[0:20]):\n",
    "    print(\"%02d. %s (%.2f)\" % (i + 1, pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will save this document-term matrix, terms, and snippets for topic modelling later using *Joblib*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T02:53:41.697391Z",
     "start_time": "2021-01-20T02:53:41.411197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cenario2_tweets-tfidf.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((A,terms,snippets), \"cenario2_tweets-tfidf.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "108px",
    "width": "338px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
